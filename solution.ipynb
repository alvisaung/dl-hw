{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18 49 58 ... 52  2  0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import io\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load the data\n",
    "with io.open('shakespeare_train.txt', 'r', encoding='utf8') as f:\n",
    "    train_text = f.read()\n",
    "\n",
    "with io.open('shakespeare_valid.txt', 'r', encoding='utf8') as f:\n",
    "    valid_text = f.read()\n",
    "\n",
    "# Create vocabulary and mappings\n",
    "vocab = sorted(set(train_text))\n",
    "vocab_size = len(vocab)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "\n",
    "\n",
    "# Convert text to integers\n",
    "train_data = np.array([vocab_to_int[c] for c in train_text], dtype=np.int32)\n",
    "valid_data = np.array([vocab_to_int[c] for c in valid_text], dtype=np.int32)\n",
    "print(train_data)\n",
    "# temp\n",
    "temp_size = 10000\n",
    "temp_size_v = 1000\n",
    "train_data = train_data[:temp_size]  # Use the first 100,000 characters\n",
    "valid_data = valid_data[:temp_size_v] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss(history):\n",
    "    plt.plot(history.history['loss'], label='Training BPC')\n",
    "    plt.plot(history.history['val_loss'], label='Validation BPC')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Bits Per Character')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpc_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "    scce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    loss = scce(y_true, y_pred)\n",
    "    loss_in_bits = loss / tf.math.log(2.0)\n",
    "    return tf.reduce_mean(loss_in_bits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(data, seq_length, batch_size):\n",
    "    # Convert data to TensorFlow Dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "    \n",
    "    # Create sequences\n",
    "    sequences = dataset.window(seq_length + 1, shift=1, drop_remainder=True)\n",
    "    sequences = sequences.flat_map(lambda window: window.batch(seq_length + 1))\n",
    "    # Split sequences into input and target\n",
    "    def split_input_target(seq):\n",
    "        input_seq = seq[:-1]\n",
    "        target_seq = seq[1:]\n",
    "        input_seq = tf.one_hot(input_seq, depth=vocab_size)\n",
    "        return input_seq, target_seq\n",
    "    \n",
    "    dataset = sequences.map(split_input_target,  num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    # Shuffle, batch, and prefetch\n",
    "    dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "seq_length = 100\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = create_tf_dataset(train_data, seq_length, batch_size)\n",
    "valid_dataset = create_tf_dataset(valid_data, seq_length, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define a custom callback to reset hidden states at the start of each epoch\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='model_epoch_{epoch}',\n",
    "    save_weights_only=True,\n",
    "    save_freq='epoch'\n",
    ")\n",
    "\n",
    "def generate_text(model, start_string, num_generate=100):\n",
    "    # Convert start string to integer indices\n",
    "    input_indices = [vocab_to_int[c] for c in start_string]\n",
    "    input_indices = tf.expand_dims(input_indices, 0)  # Add batch dimension\n",
    "\n",
    "    text_generated = []\n",
    "    model.reset_states()\n",
    "\n",
    "    for _ in range(num_generate):\n",
    "        predictions = model(input_indices)\n",
    "        predictions = tf.squeeze(predictions, 0)  # Remove batch dimension\n",
    "\n",
    "        # Use the last prediction\n",
    "        predictions = predictions[-1]\n",
    "\n",
    "        # Sample from the distribution\n",
    "        predicted_id = tf.random.categorical(predictions[None, :], num_samples=1)[0, 0].numpy()\n",
    "\n",
    "        # Add predicted character\n",
    "        text_generated.append(int_to_vocab[predicted_id])\n",
    "\n",
    "        # Update input\n",
    "        input_indices = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return start_string + ''.join(text_generated)\n",
    "\n",
    "\n",
    "def experiment(optimizer, epochs):\n",
    "    hidden_sizes = [128, 256, 512]\n",
    "    sequence_lengths = [50, 100, 200]\n",
    "\n",
    "    for hidden_units in hidden_sizes:\n",
    "        for seq_length in sequence_lengths:\n",
    "            # Build and compile the model\n",
    "            model = build_rnn_model(vocab_size, hidden_units)\n",
    "            model.compile(optimizer=optimizer, loss=bpc_loss)\n",
    "            # Train the model\n",
    "            history = model.fit(\n",
    "                train_dataset,validation_data=valid_dataset, epochs=epochs,callbacks=[checkpoint_callback]\n",
    "            )\n",
    "            # Record the final training loss\n",
    "            final_loss = history.history['loss'][-1]\n",
    "            print(f'Hidden Units: {hidden_units}, Seq Length: {seq_length}, Final Loss: {final_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_rnn_model(vocab_size):\n",
    "    embedding_dim = 256  \n",
    "    hidden_units = 1024\n",
    "    model = models.Sequential([\n",
    "        # layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim),\n",
    "        layers.Input(shape=(seq_length, vocab_size)),\n",
    "        layers.SimpleRNN(hidden_units, return_sequences=True),\n",
    "        layers.Dense(vocab_size) \n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_lstm_model(vocab_size):\n",
    "    hidden_units = 512  \n",
    "    model = models.Sequential([\n",
    "        layers.Embedding(input_dim=vocab_size, output_dim=hidden_units),\n",
    "        layers.LSTM(hidden_units, return_sequences=True, input_shape=(None, vocab_size)),\n",
    "        layers.Dense(vocab_size, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Build and compile the LSTM model\n",
    "model = build_rnn_model(vocab_size)\n",
    "\n",
    "# model = build_rnn_model(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "77/77 [==============================] - 95s 1s/step - loss: 4.8873 - val_loss: 4.7662\n",
      "Epoch 2/5\n",
      "77/77 [==============================] - 92s 1s/step - loss: 4.2419 - val_loss: 4.2245\n",
      "Epoch 3/5\n",
      "77/77 [==============================] - 91s 1s/step - loss: 3.8645 - val_loss: 4.5958\n",
      "Epoch 4/5\n",
      "77/77 [==============================] - 94s 1s/step - loss: 3.6805 - val_loss: 3.8497\n",
      "Epoch 5/5\n",
      "77/77 [==============================] - 117s 2s/step - loss: 3.4211 - val_loss: 3.7902\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.compile(optimizer='adam', loss=bpc_loss)\n",
    "epochs = 5\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JULIETh sthithpin:\n",
      "Eeowhare lngitillithe th;ales ake thithe nonth bathe sth the thous rstingore thid t mene co pulloun:\n",
      "A s:\n",
      "The d as thenon:\n",
      "MOrtheratheran:\n",
      "Firfis hest y sthenth y s wize mous y wenesthesers arsth fize m reve ounous, t, thisthen:\n",
      "Thid sthe blalicy yourersthat-\n",
      "Thimar th histhesthinglunonof the win:\n",
      "Whathar sthatheren:\n",
      "MWhed wed wes be oustocon blilin:\n",
      "US:\n",
      "whener piwigat nof odat then thesCake, ond, f fenorelid whe hollll ay\n",
      "Avesth.\n",
      "I thatherath,\n",
      "Wistenoullllillis ther y,\n",
      "Anous the la\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for model_epoch_10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated_text)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Load model weights from a specific epoch\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_epoch_10\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# For example, epoch 5\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Generate text\u001b[39;00m\n\u001b[1;32m     12\u001b[0m start_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJULIET\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/HW1/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/anaconda3/envs/HW1/lib/python3.10/site-packages/tensorflow/python/training/py_checkpoint_reader.py:31\u001b[0m, in \u001b[0;36merror_translator\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     27\u001b[0m error_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot found in checkpoint\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_message \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to find any \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatching files for\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m error_message:\n\u001b[0;32m---> 31\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors_impl\u001b[38;5;241m.\u001b[39mNotFoundError(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, error_message)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSliced checkpoints are not supported\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_message \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData type \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupported\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m error_message:\n\u001b[1;32m     36\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors_impl\u001b[38;5;241m.\u001b[39mUnimplementedError(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, error_message)\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for model_epoch_10"
     ]
    }
   ],
   "source": [
    "# Load model weights from a specific epoch\n",
    "model.load_weights('model_epoch_5')  # For example, epoch 5\n",
    "\n",
    "# Generate text\n",
    "start_string = \"JULIET\"\n",
    "generated_text = generate_text(model, start_string, num_generate=500)\n",
    "print(generated_text)\n",
    "# Load model weights from a specific epoch\n",
    "model.load_weights('model_epoch_10')  # For example, epoch 5\n",
    "\n",
    "# Generate text\n",
    "start_string = \"JULIET\"\n",
    "generated_text = generate_text(model, start_string, num_generate=500)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HW1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
