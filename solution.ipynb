{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18 49 58 ... 52  2  0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import io\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load the data\n",
    "with io.open('shakespeare_train.txt', 'r', encoding='utf8') as f:\n",
    "    train_text = f.read()\n",
    "\n",
    "with io.open('shakespeare_valid.txt', 'r', encoding='utf8') as f:\n",
    "    valid_text = f.read()\n",
    "\n",
    "# Create vocabulary and mappings\n",
    "vocab = sorted(set(train_text))\n",
    "vocab_size = len(vocab)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "\n",
    "\n",
    "# Convert text to integers\n",
    "train_data = np.array([vocab_to_int[c] for c in train_text], dtype=np.int32)\n",
    "valid_data = np.array([vocab_to_int[c] for c in valid_text], dtype=np.int32)\n",
    "print(train_data)\n",
    "# temp\n",
    "temp_size = 10000\n",
    "temp_size_v = 1000\n",
    "train_data = train_data[:]  # Use the first 100,000 characters\n",
    "valid_data = valid_data[:] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss(history):\n",
    "    plt.plot(history.history['loss'], label='Training BPC')\n",
    "    plt.plot(history.history['val_loss'], label='Validation BPC')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Bits Per Character')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpc_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "    scce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    loss = scce(y_true, y_pred)\n",
    "    loss_in_bits = loss / tf.math.log(2.0)\n",
    "    return tf.reduce_mean(loss_in_bits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(data, seq_length, batch_size):\n",
    "    # Convert data to TensorFlow Dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "    \n",
    "    # Create sequences\n",
    "    sequences = dataset.window(seq_length + 1, shift=1, drop_remainder=True)\n",
    "    sequences = sequences.flat_map(lambda window: window.batch(seq_length + 1))\n",
    "    # Split sequences into input and target\n",
    "    def split_input_target(seq):\n",
    "        input_seq = seq[:-1]\n",
    "        target_seq = seq[1:]\n",
    "        input_seq = tf.one_hot(input_seq, depth=vocab_size)\n",
    "        return input_seq, target_seq\n",
    "    \n",
    "    dataset = sequences.map(split_input_target,  num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    # Shuffle, batch, and prefetch\n",
    "    dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "seq_length = 100\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = create_tf_dataset(train_data, seq_length, batch_size)\n",
    "valid_dataset = create_tf_dataset(valid_data, seq_length, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define a custom callback to reset hidden states at the start of each epoch\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='model_epoch_{epoch}',\n",
    "    save_weights_only=True,\n",
    "    save_freq='epoch'\n",
    ")\n",
    "\n",
    "def experiment(optimizer, epochs):\n",
    "    hidden_sizes = [128, 256, 512]\n",
    "    sequence_lengths = [50, 100, 200]\n",
    "\n",
    "    for hidden_units in hidden_sizes:\n",
    "        for seq_length in sequence_lengths:\n",
    "            # Build and compile the model\n",
    "            model = build_rnn_model(vocab_size, hidden_units)\n",
    "            model.compile(optimizer=optimizer, loss=bpc_loss)\n",
    "            # Train the model\n",
    "            history = model.fit(\n",
    "                train_dataset,validation_data=valid_dataset, epochs=epochs,callbacks=[checkpoint_callback]\n",
    "            )\n",
    "            # Record the final training loss\n",
    "            final_loss = history.history['loss'][-1]\n",
    "            print(f'Hidden Units: {hidden_units}, Seq Length: {seq_length}, Final Loss: {final_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_rnn_model(vocab_size):\n",
    "    embedding_dim = 256  \n",
    "    hidden_units = 1024\n",
    "    model = models.Sequential([\n",
    "        # layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim),\n",
    "        layers.Input(shape=(seq_length, vocab_size)),\n",
    "        layers.SimpleRNN(hidden_units, return_sequences=True),\n",
    "        layers.Dense(vocab_size) \n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_lstm_model(vocab_size):\n",
    "    hidden_units = 512  \n",
    "    model = models.Sequential([\n",
    "        layers.Embedding(input_dim=vocab_size, output_dim=hidden_units),\n",
    "        layers.LSTM(hidden_units, return_sequences=True, input_shape=(None, vocab_size)),\n",
    "        layers.Dense(vocab_size, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Build and compile the LSTM model\n",
    "model = build_rnn_model(vocab_size)\n",
    "\n",
    "# model = build_rnn_model(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "    428/Unknown - 209s 480ms/step - loss: 2.7779"
     ]
    }
   ],
   "source": [
    "\n",
    "model.compile(optimizer='adam', loss=bpc_loss)\n",
    "epochs = 5\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TO BE or not to berate s thar mant alr as to,\n",
      "\n",
      "MENENIUS:\n",
      "Woy, waske hat at po core athey?\n",
      "\n",
      "Meakns wot le seramo'e he h\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_text(model, start_string, num_generate=100):\n",
    "    # Convert start string to integer indices\n",
    "    input_indices = [vocab_to_int[c] for c in start_string]\n",
    "    \n",
    "    # One-hot encode the input\n",
    "    input_one_hot = tf.one_hot(input_indices, depth=vocab_size)\n",
    "    input_one_hot = tf.expand_dims(input_one_hot, 0)  # Add batch dimension\n",
    "\n",
    "    # Ensure the input sequence has the required length\n",
    "    seq_length = model.input_shape[1]\n",
    "    if tf.shape(input_one_hot)[1] < seq_length:\n",
    "        pad_amount = seq_length - tf.shape(input_one_hot)[1]\n",
    "        input_one_hot = tf.pad(input_one_hot, [[0, 0], [pad_amount, 0], [0, 0]])\n",
    "\n",
    "\n",
    "    text_generated = []\n",
    "\n",
    "    for i in range(num_generate):\n",
    "        # Debugging: Verify input shape before prediction\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = model(input_one_hot)  # Shape: (1, seq_length, vocab_size)\n",
    "        predictions = predictions[:, -1, :]  # Get the last timestep output\n",
    "        predictions = tf.squeeze(predictions, axis=0)  # Remove batch dimension (shape: (67,))\n",
    "\n",
    "        # Sample from the distribution\n",
    "        predicted_id = tf.random.categorical(predictions[None, :], num_samples=1)[0, 0].numpy()\n",
    "\n",
    "        # Add predicted character\n",
    "        text_generated.append(int_to_vocab[predicted_id])\n",
    "\n",
    "        # Update input with the new prediction\n",
    "        next_input = tf.one_hot([predicted_id], depth=vocab_size)\n",
    "        input_one_hot = tf.concat([input_one_hot[:, 1:, :], tf.expand_dims(next_input, 0)], axis=1)\n",
    "        # Debugging\n",
    "\n",
    "    return start_string + ''.join(text_generated)\n",
    "\n",
    "\n",
    "# Load model weights from a specific epoch\n",
    "model.load_weights('model_epoch_5')  # For example, epoch 5\n",
    "\n",
    "# Generate text\n",
    "start_string = \"TO BE or not to be\"\n",
    "generated_text = generate_text(model, start_string, num_generate=100)\n",
    "print(generated_text)\n",
    "# Load model weights from a specific epoch\n",
    "# model.load_weights('model_epoch_10')  # For example, epoch 5\n",
    "\n",
    "# # Generate text\n",
    "# start_string = \"JULIET\"\n",
    "# generated_text = generate_text(model, start_string, num_generate=500)\n",
    "# print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HW1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
